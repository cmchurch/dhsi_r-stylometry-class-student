************************************
* R Stylometry - DHSI - 2019-06-10 *
************************************
https://github.com/JoannaBy/DHSI2019-Stylometry


*********
* DAY 1 *
*********

Comparing texts: whether there are relations between different texts
- authorship: can we extract some lexical features that allow us to tell who the author is?
- extend the notion to measure differences between texts
- seems to be always relational (could be adapted for year by year?)

*********
*History*
*********
- goes back to Lorenzo Valla and the Donation of Constantine (forgery) to see if it was real; allegedly written by the emperor of Constantine to give the pope the right to lands
- counted grammatical mistakes that could not have been done by a native speaker of the Greek language four centuries earlier
- feudal terminology in use throughout that wasn't in use; anachronisms throughout

1851/1887 Shakespeare authorship question -- the initial idea of measuring language
 - Augustus de Morgan (1806-187)
     - opened the question of Shakespeare authorship with a really short letter saying, 
     "hey, maybe we might look into this!"
 - Thomas C Mendenhall (1841-1924)
     - started looking into it
     
1888: authorship of Pauline Epistles
 - William Benjamin Smith (aka Conrad Mascol, 1850-1934)
 - St. Paul's Epistles written by multiple people
 
1890: chronology of Plato's dialogues:
 - Wincenty Lutoslawksi -(1864-1964) - coined term stylometry
 - assess the development of Plato's logic, and to do so he needed to know the chronology

1980s: John Burrows, book on Jane Austen - Computation and Criticism
 - bring stylometry back to the domain of literary studies
 - sparked a great number of studies looking at authorship in literary sources

********************
*WHAT IS STYLOMETRY*
********************
- language doesn't matter, but comparing across languages is really hard
- measure the words as they appear in the text, also can lemmitize
- bag o' words that needs to be counted and then sorted, and then we compare them
   - create frequency profiles and then compare them one to another
   - use dendrogram to identify groups of texts
        - Grouping of texts based on features
        - also able to identify plagiarism
          - Mysterious Mr. Robert Galbraiht - book did poorly, but turns out it was by JK
            Rowling, so then the sales sky-rocketed
   - imitation
   - translation, even if translated the signal of an author still exists in a different
     language (the Dickens signal), but is the trace of the translater still there, but it is
     rather weak
   - what are the relations between characters in a book
   
Text is not just one signal, but you have to separate the signal from the noise - key is figuring out how to isolate them and tell them apart

******************
* Considerations *
******************

Raw vs normalized
Make sure texts are roughly the same length, roughly the same number of tokens
Look for patterns

Burrow's Delta Formula (text comparison) math:
delta (T,T1) = 1/n SUM(to n, i=1)|z(fi(T))-z(fi(T1)
compared to average use in the corpus

average of averages of relative frequencies normalized to the z-score (std dev)

Differences in frequencies

closer to zero means that the texts are similar

PCA
Principal Component Analysis
- tries to position the texts in relation to each other
- principal component (a vector of features and some pattern of features)

Bootstrap Consensus Tree
vote on which connections are valid

*********
* DAY 2 *
*********

EXAMPLE
-------
Hildegard of Bingen (13th c.), got older and hired secretaries to help with writing in Latin; late in her life she gave her secretary (Gilbert of Gemlaux) the right to make any corrections he saw fit

So, who wrote the Vision of St. Martin? In an accompanying picture, Hildegard is using a wax tablet and he is using a codex, so she's taking notes and he's shaping it; she basically told him that she would provide notes, and he would know best how to turn it into good writing

Bernardus Silvestris, Hildegard and Guibert -- used early component analysis to compare three authors (Bernadus as control, with Hildegard and Guibert) -- a selection of 6 lemmitized function words in Latin to make comparisons

USING PCA, you can see that the Vision of St. Martin was actually written by her secretary (Gilbert); he put her name on the title page because he knew her celebrity would sell the work -- but we also see a FOURTH author, which is a combination of Hildegard and Guibert's style (a virtual author whose style was defined by H.'s writing and G.'s editing).


MATH
----
Culling - what to filter out
https://joannaby.github.io/Culling/Culling.htm
 - you can filter out proper names, because we don't really need them to distinguish between authors, only really between the texts -- way too specific
 
Types vs Tokens
 - tokens - each occurence of word is unique, so repetitions are included as tokens
 - types - each occurence of the word "type" is unique -- do not repeat the same 	word multiple times

Problem of tails - huge tail of words that are not terribly helpful, because they only appear once in the text

Chosing words that tell us about meaningful differences

Culling: excluding some words (tokens/types) from our frequency tables, so that they are not included in the analysis -- "automatic manipulation of the word list" - exact percentage of the words that we do not want to include in the text comparison
 -- so, a culling of 20% would mean that only the words that appear in at least 20% of the texts will be included in the wordlist
 -- can be used to filter out some genre specific terms

DOS and DON'TS 
--------------
DO:
* Remember: the higher the culling, the fewer MFW on your list.
* Carefully think what % of culling will be useful
* usually 20-50% is fine for excluding single works’ noise,
* higher values should be applied for very specific uses.
* Compare results with and without culling.

TFIDF scoring - term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus
   -- The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word

TOPIC MODELLING - options
 - invoke mallet as package in R
 - also could use topicmodels package in R
 - finally, there is the Dariah TopicExplorer program, which is very beginner friendly
 
 - you can do clustering accroding to the proportions of topics
 
 
DISTANCE MEASURES
-----------------
Zipf's law 
 - the frequency of any word is inversely proportional to its rank in the frequency table (the more frequent a word, the more common it is)
   - function words at the top (the, a, and)
   - common words (made, miss, too, sir, dear, make) at the middle
   - distintive words (abel, accomodations, acre, addicted, advertisement) at the bottom
 - the more frequent a word, the more meanings it has
   - "drink" can be verb or noun, "round" can be verb, noun, adjective
 - principle of least effort - don't want to create new words, so we use what we already have -- so with writing processes, we most generally reach for words that we know and use often, while more stylistically or "lexically rich" words are used less frequently (exponentially so)
 - the long tail of the distribution

Lexical Richness
 - Comparing Types vs Tokens (how many unique words vs how many total tokens) to show lexical richness

Are all words equally important?
 - process of analysis consists of 
    1. selecting words
	2. creating table of frequencies
	3. using distance measures to assess similarity

- HOW TO MEASURE "DISTANCE"
 - number of words you use deterimines the number of dimensions (we can have a multidimensional space when doing the math) -- using geometry as a way to count the differences between words (i.e. converting them to vectors)
	- Using 1 dimension: the difference in absolute frequency between the two texts (only x axis)
	- Two dimensional space could compare two words (x,y axis)
	- Three dimensional space could compare three words (x,y,z axis)
	- But what about 100 words with 100 dimensions?
	- we wrinkle time using two multidimensional spaces (imagined geometries)
		- using a table (matrix) of distances (clustering, MDS - multi-dimensional scaling)
		- rotating a multidimensional space (PCA)

PCA - cover only some components of the information (PCA is a lossy format)
 - basic concept: looking at data to find coordinates that represent the biggest amount of information -- finding the most optimal representation
     -e.g. rotating a 3d kettle on a computer screen (2d) to show to most accurate representation of what a kettle is
 - Components in PCA are sets of features which grant best information preservation, note however that they always havea  value covering a relatively small percentage of information
 - represent the most distinctive features
 - privileges the features that have the greatest spread (demonstrates the largest variance of the data)
    - creating an artifical vector using a combination of a certain number of words (so the number of dimensions are identical to the number of words)
	- in PCA, each axis has a percentage of "features" that shows how many words contributed to the vector created on the X axis, and again on the Y axis
 - PCA does not rely on the distance of the words at all, it only rotates the multidimensional space
	
	Mp3 is PCA on musical data
 
 *** YOU HAVE TO KNOW YOUR MATERIAL CLOSELY ***
 
Calculating the distances between two words (and two texts)
|a1 - b1| (absolute value of the frequency of the word between two texts)
|a1 - b1| + |a2 - b2| + |a3 - b3| .... (sum up the differences)

Stylometric disntances as political systems
DISTANCE MEASURE				POLITICAL SYSTEM
Euclidean disntance 	=> 		tyranny
Manhattan distance		=>		oligarchy

Euclidean distance
------------------
take the frequency of the word in a text, then take another frequency

Generally advised to use the distance measures using Z-scores (Burrows, Edel, etc.)
 - twice normalized (against total tokens and against std deviation)

*******************
* AFTERNOON DAY 2 *
*******************

Example
-------
Greek New Testament
- Synoptic Gospels (c. 50% common material)
   - Mark (c. 68-73), Matthew (c. 70-100), Luke (c. 80-100)
- John (c. 90-100) - gospel developed from a Johannine circle
- Acts (c. 64-69) traditionally attributed to St. Luke

Bootstrapping
- way of thinking, and a way to get around cherry picking
- replicating your study many times, each time with slightly different parameters
	- intensive resampling of the original data
	- used when original data is not sufficient
	- or when the distribution is not normal (i.e. Gaussian)
- Baron Munchausen, pulled himself out of a swamp by his hair (German) or by his own bootstraps (English) without anyone's help
- iterate multiple times with slightly different parameters, then create a dendrogam
 - creates a confidence interval based on how many times the connection reappear, which then determines the break points of the dendrogram
 - each iteration "votes" for each potential link, which then leads to a consesnus tree; all "votes" look for the connection to surpass a particular p value (i.e. p=0.5 means that the links only display for those that showed up in 50% of iterations)
 - basically, build and compare multiple networked versions of the data based on data/ties and then prune it down to only the strongest (most robust) ties, then create dendrogram from that
 - a pruning procedure

NETWORK
 - In stylo, you run a consensus iteration of cluster analysis, building a network for each one.
    - edge weights are the summation of the number of networks with that tie
 - networks are useless, though pretty, unless we ask a good research question
  
